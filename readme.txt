# model transformation

Let x âˆˆ R nÃ—d denote the input where n is the sequence length (e.g., the number of tokens) and d is the model dimension.
We define a Transformer block as a function fÎ¸ : R^nÃ—d â†’ R^nÃ—d with trainable parameters Î¸. Then the Transformer
inference, i.e., fÎ¸(x) = y, is computed as follows:
    Q = x Wq, K = x Wk, V = x Wv, where Wq, Wk, Wv âˆˆ R^dÃ—d

    u = softmax (Q K' / âˆšk + M) V Wo, where M âˆˆ R^nÃ—n, Wo âˆˆ R^dÃ—d
 
    v = LayerNorm(u + x; Î³1, Î²1), where Î³1, Î²1 âˆˆ R^d

    z = ReLU(v W1)W2, W1 âˆˆ R where dÃ—m, W2 âˆˆ R^mÃ—d

    y = LayerNorm(z + v; Î³2, Î²2), where Î³1, Î²1 âˆˆ R^d

where k is a constant equal to d divided by the number of attention heads, M denotes the mask which is an all-zero
matrix in the encoder and a matrix whose upper right corner (not including the diagonal) is negative infinity in
the decoder. The parameter Î¸ consists of attention weights (Wq, Wk, Wv, Wo), feedforward weights (W1, W2) and
LayerNorm weights (Î³, Î²).

Let Ï€ âˆˆ {0, 1} dÃ—d denote a permutation matrix. We transform the parameters Î¸ as follows:
Wqâ€² = Ï€áµ€ Wq 
Wkâ€² = Ï€áµ€ Wk
Wvâ€² = Ï€áµ€ Wv 
W1â€² = Ï€áµ€ W1
Woâ€² = Wo Ï€
W2â€² = W2 Ï€
Î³1â€² = Î³1 Ï€
Î²1â€² = Î²1 Ï€ 
Î³2â€² = Î³2 Ï€ 
Î²2â€² = Î²2 Ï€

FFN_SwiGLU(x) = ( (x Wâ‚) Â· Ïƒ(x Wâ‚) âŠ™ (x Wâ‚ƒ) ) Wâ‚‚. where ð‘Š1,ð‘Š3 âˆˆ R^ð‘‘Ã—ð‘š,ð‘Š2 âˆˆ R^ð‘šÃ—ð‘‘

W1â€² = Ï€áµ€ W1
W3â€² = Ï€áµ€ W3
W2â€² = W2 Ï€


Recall that SiLU(z) = z Â· Ïƒ(z), so the inner term
â€ƒâ€ƒSiLU(xWâ‚) âŠ™ (xWâ‚ƒ)

self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))

gate_proj = W1
down_proj = W2
up_proj = W3

Thus, the code implements:
â€ƒFFNSwiGLU(x) = ( SiLU(xWâ‚) âŠ™ (xWâ‚ƒ) ) Wâ‚‚

â€ƒ
Pytorch linear layer implements stores internally Wáµ€ and implements y = xW + b 

as

y = x(Wáµ€)áµ€ + b


so in code we use identity (AB)áµ€ = Báµ€ Aáµ€

For example given the equation Ï€áµ€ W1 we set  W1áµ€ Ï€ 


# inference

at inference it works as follows:

1. transform x to xâ€² = x Ï€
2. compute fÎ¸â€²(xâ€²) = yâ€²
3. compute y = yâ€² Ï€áµ€


since x Ï€ Wqâ€² =  x Ï€ Ï€áµ€ Wq = x I Wq = x Wq (unmodified query), the same is true for Values and Keys kv-cache will contain the same content as if original model is used on non-permuted prompt.

we need to emulate the situation when the host where llm inference is taking place don't have access to Ï€ or original Î¸,
but currently last layer of LlamaForCausalLM that computes logits performs reverse transformation, so the "host" has access to llm output.
we can hide this computation from the "host" but we will have to perform permuted-features -> logits transformation on users' side.
